{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b8fb167-71e6-48ef-ad63-01c3cc954e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\khush\\onedrive\\desktop\\infocrucible\\.venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\khush\\onedrive\\desktop\\infocrucible\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\khush\\onedrive\\desktop\\infocrucible\\.venv\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: click in c:\\users\\khush\\onedrive\\desktop\\infocrucible\\.venv\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\khush\\onedrive\\desktop\\infocrucible\\.venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\khush\\onedrive\\desktop\\infocrucible\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03775e46-e71d-41af-8bf3-7047ed9f9d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\khush\\onedrive\\desktop\\infocrucible\\.venv\\lib\\site-packages (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy --only-binary=:all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f9391e7-37b4-493b-b8a4-84ad33b0b731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\khush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\khush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import standard libraries\n",
    "import os, io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Azure Cosmos DB client\n",
    "from azure.cosmos import CosmosClient\n",
    "\n",
    "# Azure Blob Storage client\n",
    "import nltk\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "\n",
    "# Deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout, Input, concatenate\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Ensure NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ed22cbe-7dae-48df-9554-aa1a91a11dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 26599 records from Cosmos DB.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id\n",
       "0  0\n",
       "1  1\n",
       "2  3\n",
       "3  4\n",
       "4  5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "\n",
    "config    = dotenv_values(\".env\")\n",
    "COSMOS_URL = config[\"COSMOS_ENDPOINT\"]\n",
    "COSMOS_KEY = config[\"COSMOS_KEY\"]\n",
    "DATABASE_NAME = config[\"COSMOS_DATABASE\"]\n",
    "CONTAINER_NAME = config[\"COSMOS_CONTAINER\"]\n",
    "\n",
    "# Connect to Cosmos DB\n",
    "cosmos_client = CosmosClient(COSMOS_URL, credential=COSMOS_KEY)\n",
    "database = cosmos_client.get_database_client(DATABASE_NAME)\n",
    "container = database.get_container_client(CONTAINER_NAME)\n",
    "\n",
    "# Query all items (select relevant fields)\n",
    "query = \"SELECT c.id, c.statement, c.image, c.web, c.category, c.date, c.label FROM c\"\n",
    "items = list(container.query_items(\n",
    "    query=query,\n",
    "    enable_cross_partition_query=True\n",
    "))\n",
    "df = pd.DataFrame(items)\n",
    "print(f\"Loaded {len(df)} records from Cosmos DB.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90451946-0855-4e15-85c8-43bf4d18884e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Incomplete download: ('Connection broken: IncompleteRead(15721 bytes read, 105837 more expected)', IncompleteRead(15721 bytes read, 105837 more expected))\n",
      "Incomplete download: ('Connection broken: IncompleteRead(32105 bytes read, 196197 more expected)', IncompleteRead(32105 bytes read, 196197 more expected))\n"
     ]
    },
    {
     "ename": "ServiceRequestError",
     "evalue": "<urllib3.connection.HTTPSConnection object at 0x0000029B9F614340>: Failed to resolve 'infocrucible.blob.core.windows.net' ([Errno 11001] getaddrinfo failed)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mServiceRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m     download_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_dir, blob\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(download_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 20\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(\u001b[43mblob_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_blob\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreadall())\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(container_client\u001b[38;5;241m.\u001b[39mlist_blobs()))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m blobs to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\InfoCrucible\\.venv\\lib\\site-packages\\azure\\core\\tracing\\decorator.py:119\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# If tracing is disabled globally and user didn't explicitly enable it, don't trace.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_enabled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m tracing_enabled \u001b[38;5;129;01mand\u001b[39;00m user_enabled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\InfoCrucible\\.venv\\lib\\site-packages\\azure\\storage\\blob\\_blob_client.py:753\u001b[0m, in \u001b[0;36mBlobClient.download_blob\u001b[1;34m(self, offset, length, encoding, **kwargs)\u001b[0m\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomer provided encryption key must be used over HTTPS.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    736\u001b[0m options \u001b[38;5;241m=\u001b[39m _download_blob_options(\n\u001b[0;32m    737\u001b[0m     blob_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblob_name,\n\u001b[0;32m    738\u001b[0m     container_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    751\u001b[0m     client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client,\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 753\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StorageStreamDownloader(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\InfoCrucible\\.venv\\lib\\site-packages\\azure\\storage\\blob\\_download.py:403\u001b[0m, in \u001b[0;36mStorageStreamDownloader.__init__\u001b[1;34m(self, clients, config, start_range, end_range, validate_content, encryption_options, max_concurrency, name, container, encoding, download_cls, **kwargs)\u001b[0m\n\u001b[0;32m    393\u001b[0m     initial_request_end \u001b[38;5;241m=\u001b[39m initial_request_start \u001b[38;5;241m+\u001b[39m first_get_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_range, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_offset \u001b[38;5;241m=\u001b[39m process_range_and_offset(\n\u001b[0;32m    396\u001b[0m     initial_request_start,\n\u001b[0;32m    397\u001b[0m     initial_request_end,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encryption_data\n\u001b[0;32m    401\u001b[0m )\n\u001b[1;32m--> 403\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initial_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproperties \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlobProperties\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response\u001b[38;5;241m.\u001b[39mproperties)\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproperties\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\InfoCrucible\\.venv\\lib\\site-packages\\azure\\storage\\blob\\_download.py:456\u001b[0m, in \u001b[0;36mStorageStreamDownloader._initial_request\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m retry_active:\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 456\u001b[0m         location_mode, response \u001b[38;5;241m=\u001b[39m cast(Tuple[Optional[\u001b[38;5;28mstr\u001b[39m], Any], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clients\u001b[38;5;241m.\u001b[39mblob\u001b[38;5;241m.\u001b[39mdownload(\n\u001b[0;32m    457\u001b[0m             \u001b[38;5;28mrange\u001b[39m\u001b[38;5;241m=\u001b[39mrange_header,\n\u001b[0;32m    458\u001b[0m             range_get_content_md5\u001b[38;5;241m=\u001b[39mrange_validation,\n\u001b[0;32m    459\u001b[0m             validate_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_content,\n\u001b[0;32m    460\u001b[0m             data_stream_total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    461\u001b[0m             download_stream_current\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    462\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_options\n\u001b[0;32m    463\u001b[0m         ))\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;66;03m# Check the location we read from to ensure we use the same one\u001b[39;00m\n\u001b[0;32m    466\u001b[0m         \u001b[38;5;66;03m# for subsequent requests.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_location_mode \u001b[38;5;241m=\u001b[39m location_mode\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\InfoCrucible\\.venv\\lib\\site-packages\\azure\\core\\tracing\\decorator.py:119\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# If tracing is disabled globally and user didn't explicitly enable it, don't trace.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_enabled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m tracing_enabled \u001b[38;5;129;01mand\u001b[39;00m user_enabled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\InfoCrucible\\.venv\\lib\\site-packages\\azure\\storage\\blob\\_generated\\operations\\_blob_operations.py:1641\u001b[0m, in \u001b[0;36mBlobOperations.download\u001b[1;34m(self, snapshot, version_id, timeout, range, range_get_content_md5, range_get_content_crc64, structured_body_type, request_id_parameter, lease_access_conditions, cpk_info, modified_access_conditions, **kwargs)\u001b[0m\n\u001b[0;32m   1639\u001b[0m _decompress \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecompress\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1640\u001b[0m _stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 1641\u001b[0m pipeline_response: PipelineResponse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_pipeline\u001b[38;5;241m.\u001b[39mrun(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1642\u001b[0m     _request, stream\u001b[38;5;241m=\u001b[39m_stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1643\u001b[0m )\n\u001b[0;32m   1645\u001b[0m response \u001b[38;5;241m=\u001b[39m pipeline_response\u001b[38;5;241m.\u001b[39mhttp_response\n\u001b[0;32m   1647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m206\u001b[39m]:\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\InfoCrucible\\.venv\\lib\\site-packages\\azure\\core\\pipeline\\_base.py:242\u001b[0m, in \u001b[0;36mPipeline.run\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    240\u001b[0m pipeline_request: PipelineRequest[HTTPRequestType] \u001b[38;5;241m=\u001b[39m PipelineRequest(request, context)\n\u001b[0;32m    241\u001b[0m first_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_policies[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_policies \u001b[38;5;28;01melse\u001b[39;00m _TransportRunner(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport)\n\u001b[1;32m--> 242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfirst_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_request\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\InfoCrucible\\.venv\\lib\\site-packages\\azure\\core\\pipeline\\_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     96\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\InfoCrucible\\.venv\\lib\\site-packages\\azure\\core\\pipeline\\_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     96\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "    \u001b[1;31m[... skipping similar frames: _SansIOHTTPPolicyRunner.send at line 98 (2 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\InfoCrucible\\.venv\\lib\\site-packages\\azure\\core\\pipeline\\_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     96\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\InfoCrucible\\.venv\\lib\\site-packages\\azure\\core\\pipeline\\policies\\_redirect.py:205\u001b[0m, in \u001b[0;36mRedirectPolicy.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    203\u001b[0m original_domain \u001b[38;5;241m=\u001b[39m get_domain(request\u001b[38;5;241m.\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39murl) \u001b[38;5;28;01mif\u001b[39;00m redirect_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m retryable:\n\u001b[1;32m--> 205\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m     redirect_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_redirect_location(response)\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m redirect_location \u001b[38;5;129;01mand\u001b[39;00m redirect_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\InfoCrucible\\.venv\\lib\\site-packages\\azure\\core\\pipeline\\_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     96\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\InfoCrucible\\.venv\\lib\\site-packages\\azure\\storage\\blob\\_shared\\policies.py:555\u001b[0m, in \u001b[0;36mStorageRetryPolicy.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    553\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msleep(retry_settings, request\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39mtransport)\n\u001b[0;32m    554\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 555\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retry_settings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    557\u001b[0m     response\u001b[38;5;241m.\u001b[39mcontext[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m retry_settings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\InfoCrucible\\.venv\\lib\\site-packages\\azure\\storage\\blob\\_shared\\policies.py:527\u001b[0m, in \u001b[0;36mStorageRetryPolicy.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m retries_remaining:\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 527\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    528\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_retry(response, retry_settings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;129;01mor\u001b[39;00m is_checksum_retry(response):\n\u001b[0;32m    529\u001b[0m             retries_remaining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[0;32m    530\u001b[0m                 retry_settings,\n\u001b[0;32m    531\u001b[0m                 request\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mhttp_request,\n\u001b[0;32m    532\u001b[0m                 response\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mhttp_response)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\InfoCrucible\\.venv\\lib\\site-packages\\azure\\core\\pipeline\\_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     96\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\InfoCrucible\\.venv\\lib\\site-packages\\azure\\core\\pipeline\\_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     96\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "    \u001b[1;31m[... skipping similar frames: _SansIOHTTPPolicyRunner.send at line 98 (1 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\InfoCrucible\\.venv\\lib\\site-packages\\azure\\core\\pipeline\\_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     96\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\InfoCrucible\\.venv\\lib\\site-packages\\azure\\storage\\blob\\_shared\\policies.py:301\u001b[0m, in \u001b[0;36mStorageResponseHook.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    296\u001b[0m     upload_stream_current \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupload_stream_current\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    298\u001b[0m response_callback \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse_callback\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[0;32m    299\u001b[0m     request\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_response_hook\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_callback)\n\u001b[1;32m--> 301\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    303\u001b[0m will_retry \u001b[38;5;241m=\u001b[39m is_retry(response, request\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m is_checksum_retry(response)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m# Auth error could come from Bearer challenge, in which case this request will be made again\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\InfoCrucible\\.venv\\lib\\site-packages\\azure\\core\\pipeline\\_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     96\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\InfoCrucible\\.venv\\lib\\site-packages\\azure\\core\\pipeline\\_base.py:98\u001b[0m, in \u001b[0;36m_SansIOHTTPPolicyRunner.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     96\u001b[0m _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_request, request)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     _await_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy\u001b[38;5;241m.\u001b[39mon_exception, request)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\InfoCrucible\\.venv\\lib\\site-packages\\azure\\core\\pipeline\\_base.py:130\u001b[0m, in \u001b[0;36m_TransportRunner.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"HTTP transport send method.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \n\u001b[0;32m    122\u001b[0m \u001b[38;5;124;03m:param request: The PipelineRequest object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03m:rtype: ~azure.core.pipeline.PipelineResponse\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    127\u001b[0m cleanup_kwargs_for_transport(request\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m PipelineResponse(\n\u001b[0;32m    129\u001b[0m     request\u001b[38;5;241m.\u001b[39mhttp_request,\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sender\u001b[38;5;241m.\u001b[39msend(request\u001b[38;5;241m.\u001b[39mhttp_request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39moptions),\n\u001b[0;32m    131\u001b[0m     context\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mcontext,\n\u001b[0;32m    132\u001b[0m )\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\InfoCrucible\\.venv\\lib\\site-packages\\azure\\storage\\blob\\_shared\\base_client.py:353\u001b[0m, in \u001b[0;36mTransportWrapper.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\InfoCrucible\\.venv\\lib\\site-packages\\azure\\storage\\blob\\_shared\\base_client.py:353\u001b[0m, in \u001b[0;36mTransportWrapper.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\InfoCrucible\\.venv\\lib\\site-packages\\azure\\core\\pipeline\\transport\\_requests_basic.py:409\u001b[0m, in \u001b[0;36mRequestsTransport.send\u001b[1;34m(self, request, proxies, **kwargs)\u001b[0m\n\u001b[0;32m    406\u001b[0m     error \u001b[38;5;241m=\u001b[39m ServiceRequestError(err, error\u001b[38;5;241m=\u001b[39merr)\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error:\n\u001b[1;32m--> 409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_rest(request):\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_requests_basic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RestRequestsTransportResponse\n",
      "\u001b[1;31mServiceRequestError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x0000029B9F614340>: Failed to resolve 'infocrucible.blob.core.windows.net' ([Errno 11001] getaddrinfo failed)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Use your Azure Storage connection string\n",
    "CONNECTION_STRING =config[\"STORAGE_CONN_STR\"]\n",
    "BLOB_CONTAINER = config[\"STORAGE_CONTAINER\"]\n",
    "\n",
    "# Connect to Blob service via connection string\n",
    "blob_service_client = BlobServiceClient.from_connection_string(CONNECTION_STRING)\n",
    "container_client = blob_service_client.get_container_client(BLOB_CONTAINER)\n",
    "\n",
    "# Download all blobs (or only those referenced by df) into a local folder\n",
    "image_dir = \"downloaded_images\"\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "for blob in container_client.list_blobs():\n",
    "    blob_client = container_client.get_blob_client(blob)\n",
    "    download_path = os.path.join(image_dir, blob.name)\n",
    "    with open(download_path, \"wb\") as f:\n",
    "        f.write(blob_client.download_blob().readall())\n",
    "\n",
    "print(f\"Downloaded {len(list(container_client.list_blobs()))} blobs to '{image_dir}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41fe7684-f8b4-4002-93fd-7c9a4fce8f44",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SnowballStemmer\n\u001b[0;32m      4\u001b[0m stemmer \u001b[38;5;241m=\u001b[39m SnowballStemmer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnormalize_text\u001b[39m(text):\n\u001b[0;32m      8\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)   # remove punctuation\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [stemmer.stem(t) for t in tokens if t not in stop_words and t.isalpha()]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply normalization\n",
    "df['clean_statement'] = df['statement'].astype(str).apply(normalize_text)\n",
    "df = df.dropna(subset=['clean_statement'])\n",
    "print(\"Sample normalized text:\", df['clean_statement'].iloc[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab1c7d74-ebe2-4c50-afc0-afe9529ab14d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "Index(['clean_statement'], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14396\\1961791448.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Remove exact duplicate statements\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbefore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_statement'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mafter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Dropped {before-after} duplicate statements; {after} remain.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Desktop\\InfoCrucible\\.venv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, subset, keep, inplace, ignore_index)\u001b[0m\n\u001b[0;32m   6821\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6822\u001b[0m         \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"inplace\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6823\u001b[0m         \u001b[0mignore_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ignore_index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6825\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6826\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6827\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefault_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6828\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Desktop\\InfoCrucible\\.venv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, subset, keep)\u001b[0m\n\u001b[0;32m   6953\u001b[0m         \u001b[1;31m# Otherwise, raise a KeyError, same as if you try to __getitem__ with a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6954\u001b[0m         \u001b[1;31m# key that doesn't exist.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6955\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6956\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6957\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6959\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6960\u001b[0m             \u001b[1;31m# GH#45236 This is faster than get_group_index below\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: Index(['clean_statement'], dtype='object')"
     ]
    }
   ],
   "source": [
    "# Remove exact duplicate statements\n",
    "before = len(df)\n",
    "df = df.drop_duplicates(subset=['clean_statement']).reset_index(drop=True)\n",
    "after = len(df)\n",
    "print(f\"Dropped {before-after} duplicate statements; {after} remain.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fc9b90-90e5-45f8-8878-4684e06ca352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Separate real vs fake for augmentation\n",
    "real_df = df[df['label']=='real'].reset_index(drop=True)\n",
    "fake_df = df[df['label']=='fake'].reset_index(drop=True)\n",
    "\n",
    "# Vectorize fake statements with TF-IDF\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "fake_tfidf = tfidf_vec.fit_transform(fake_df['clean_statement'])\n",
    "\n",
    "# Compute cosine similarity among fake statements\n",
    "sim_matrix = cosine_similarity(fake_tfidf)\n",
    "\n",
    "# Threshold for similarity (e.g., 0.8); combine sentences with high similarity\n",
    "augmented_texts = []\n",
    "threshold = 0.8\n",
    "for i in range(len(fake_df)):\n",
    "    for j in range(i+1, len(fake_df)):\n",
    "        if sim_matrix[i,j] > threshold:\n",
    "            # Combine unique words from both sentences\n",
    "            words_i = set(fake_df.loc[i, 'clean_statement'].split())\n",
    "            words_j = set(fake_df.loc[j, 'clean_statement'].split())\n",
    "            combined = \" \".join(list(words_i.union(words_j)))\n",
    "            augmented_texts.append(combined)\n",
    "\n",
    "# Create new fake entries from augmented texts\n",
    "aug_df = pd.DataFrame({\n",
    "    'id': [f\"aug_{k}\" for k in range(len(augmented_texts))],\n",
    "    'clean_statement': augmented_texts,\n",
    "    'image': None,\n",
    "    'web': None,\n",
    "    'category': None,\n",
    "    'date': None,\n",
    "    'label': 'fake'\n",
    "})\n",
    "print(f\"Generated {len(augmented_texts)} augmented fake statements.\")\n",
    "df_augmented = pd.concat([df, aug_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d2218f-b09c-4897-b47d-1360c3d39215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize for LDA\n",
    "tokenized = [text.split() for text in df_augmented['clean_statement']]\n",
    "dictionary = corpora.Dictionary(tokenized)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized]\n",
    "\n",
    "# Train LDA model (e.g., 5 topics)\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=10)\n",
    "for idx, topic in lda_model.print_topics(num_words=5):\n",
    "    print(f\"Topic {idx}: {topic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4933a09a-463f-43cb-aad7-ed36f110f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF features (unigram)\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = tfidf.fit_transform(df_augmented['clean_statement']).toarray()\n",
    "\n",
    "# Sentence embeddings using DistilBERT\n",
    "st_model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "X_emb = st_model.encode(df_augmented['clean_statement'], show_progress_bar=True)\n",
    "print(f\"Text embeddings shape: {X_emb.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f393f911-a1af-417a-b978-c029ddb41889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ResNet-50 model for feature extraction (no classification head)\n",
    "resnet = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "def extract_image_features(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    features = resnet.predict(x)\n",
    "    return features.flatten()\n",
    "\n",
    "# Map image filenames to features\n",
    "img_features = {}\n",
    "for fname in df_augmented['image'].dropna().unique():\n",
    "    path = os.path.join(image_dir, fname)\n",
    "    if os.path.exists(path):\n",
    "        img_features[fname] = extract_image_features(path)\n",
    "print(f\"Extracted features for {len(img_features)} images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcda0df-1e72-48cf-9be8-aea026771d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge image features into dataframe (for statements with images)\n",
    "df_augmented['img_feat'] = df_augmented['image'].map(lambda fname: img_features.get(fname))\n",
    "# Fill missing image features with zeros (for simplicity)\n",
    "no_image_vec = np.zeros(resnet.output_shape[-1])\n",
    "df_augmented['img_feat'] = df_augmented['img_feat'].apply(lambda x: x if isinstance(x, np.ndarray) else no_image_vec)\n",
    "\n",
    "# Feature matrices\n",
    "X_text_tfidf = X_tfidf  # (n_samples, n_features)\n",
    "X_text_emb = X_emb     # (n_samples, emb_dim)\n",
    "X_image = np.stack(df_augmented['img_feat'].values)\n",
    "y = (df_augmented['label'] == 'fake').astype(int).values  # binary labels\n",
    "\n",
    "# Combine TF-IDF and image for one set, embeddings and image for another, etc.\n",
    "X_tfidf_img = np.hstack([X_text_tfidf, X_image])\n",
    "X_emb_img = np.hstack([X_text_emb, X_image])\n",
    "\n",
    "# Train/test split\n",
    "X_tfidf_img_train, X_tfidf_img_test, y_train, y_test = train_test_split(X_tfidf_img, y, test_size=0.2, random_state=42)\n",
    "X_emb_img_train, X_emb_img_test, _, _ = train_test_split(X_emb_img, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e35659-96c3-4ac2-a2b2-312cf66a13d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Logistic Regression on TF-IDF alone\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_text_tfidf[y_train.index], y_train)   # text only (if we separated earlier)\n",
    "y_pred_lr = lr.predict(X_text_tfidf[y_test.index])\n",
    "print(\"Logistic Regression (text only):\", accuracy_score(y_test, y_pred_lr))\n",
    "\n",
    "# Decision Tree on TF-IDF + image\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_tfidf_img_train, y_train)\n",
    "y_pred_dt = dt.predict(X_tfidf_img_test)\n",
    "print(\"Decision Tree (TF-IDF+image):\", accuracy_score(y_test, y_pred_dt))\n",
    "\n",
    "# Naive Bayes on TF-IDF\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_tfidf_img_train, y_train)  # Note: MultinomialNB expects non-negative features\n",
    "y_pred_nb = nb.predict(X_tfidf_img_test)\n",
    "print(\"Naive Bayes (TF-IDF+image):\", accuracy_score(y_test, y_pred_nb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4239aa6b-ed19-43e1-9b3e-7ddd642bfa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare text sequences for LSTM\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(df_augmented['clean_statement'])\n",
    "seqs = tokenizer.texts_to_sequences(df_augmented['clean_statement'])\n",
    "seqs_padded = pad_sequences(seqs, maxlen=100)\n",
    "X_seq = seqs_padded\n",
    "X_seq_train, X_seq_test, _, _ = train_test_split(X_seq, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# LSTM model\n",
    "input_layer = Input(shape=(100,))\n",
    "x = Embedding(input_dim=10000, output_dim=128)(input_layer)\n",
    "x = LSTM(64)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_layer = Dense(1, activation='sigmoid')(x)\n",
    "lstm_model = Model(input_layer, output_layer)\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "lstm_model.fit(X_seq_train, y_train, epochs=2, batch_size=32, validation_split=0.1)  # brief training\n",
    "y_pred_lstm = (lstm_model.predict(X_seq_test) > 0.5).astype(int)\n",
    "print(\"LSTM Accuracy:\", accuracy_score(y_test, y_pred_lstm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600089ce-0363-4254-9764-4c7bfd1ae503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-LSTM model\n",
    "input_layer = Input(shape=(100,))\n",
    "x = Embedding(input_dim=10000, output_dim=128)(input_layer)\n",
    "x = Bidirectional(LSTM(64))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_layer = Dense(1, activation='sigmoid')(x)\n",
    "bilstm_model = Model(input_layer, output_layer)\n",
    "bilstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "bilstm_model.fit(X_seq_train, y_train, epochs=2, batch_size=32, validation_split=0.1)\n",
    "y_pred_bilstm = (bilstm_model.predict(X_seq_test) > 0.5).astype(int)\n",
    "print(\"Bi-LSTM Accuracy:\", accuracy_score(y_test, y_pred_bilstm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b253c938-c015-4ead-b8d3-79ca3dc327cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Use pre-trained VGG16 for image classification (pseudo-code)\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "vgg = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
    "# Extract features for images and train a small neural network classifier on top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c645eb-fde5-4c1e-b2d7-a08e34cedf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "# Example MLP on combined text embedding + image feature\n",
    "text_input = Input(shape=(X_emb.shape[1],))\n",
    "img_input = Input(shape=(X_image.shape[1],))\n",
    "merged = Concatenate()([text_input, img_input])\n",
    "x = Dense(256, activation='relu')(merged)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "mlp = Model([text_input, img_input], output)\n",
    "mlp.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Prepare inputs for fusion model\n",
    "X_comb_train = [X_text_emb[y_train.index], X_image[y_train.index]]\n",
    "X_comb_test = [X_text_emb[y_test.index], X_image[y_test.index]]\n",
    "mlp.fit(X_comb_train, y_train, epochs=2, batch_size=32, validation_split=0.1)\n",
    "y_pred_mlp = (mlp.predict(X_comb_test) > 0.5).astype(int)\n",
    "print(\"Fusion MLP Accuracy:\", accuracy_score(y_test, y_pred_mlp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4f7463-9f65-430d-b1e0-d722379973ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Classification Report for Fusion MLP:\")\n",
    "print(classification_report(y_test, y_pred_mlp))\n",
    "cm = confusion_matrix(y_test, y_pred_mlp)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5754f4-0dff-48cd-8992-7ddd5b01a813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert documents into Azure Cosmos Vector DB\n",
    "from azure.cosmos import CosmosClient\n",
    "from azure.cosmos.partition_key import PartitionKey\n",
    "\n",
    "# Create a container with vector indexing enabled (requires Azure setup)\n",
    "vector_container = database.get_container_client(\"vector_store\")\n",
    "for idx, row in df_augmented.iterrows():\n",
    "    embedding = st_model.encode(row['clean_statement']).tolist()\n",
    "    vector_container.upsert_item({\n",
    "        'id': str(row['id']),\n",
    "        'statement': row['clean_statement'],\n",
    "        'label': row['label'],\n",
    "        'embedding': embedding  # stored as a vector type\n",
    "    })\n",
    "\n",
    "#  RAG Query: Given a statement, retrieve similar docs and call LLM\n",
    "def rag_fact_check(query_text):\n",
    "    query_emb = st_model.encode([query_text]).tolist()[0]\n",
    "    sql_query = {\n",
    "        'query': \"SELECT TOP 3 c.statement FROM c ORDER BY ST_DISTANCE(c.embedding, @q) ASC\",\n",
    "        'parameters': [{'name': '@q', 'value': query_emb}]\n",
    "    }\n",
    "    docs = list(vector_container.query_items(query=sql_query, enable_cross_partition_query=True))\n",
    "    context = \"\\n\\n\".join([d['statement'] for d in docs])\n",
    "    # Use OpenAI Completion with context\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(api_key=\"<YOUR_OPENAI_KEY>\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-preview\",\n",
    "        messages=[\n",
    "            {\"role\":\"system\", \"content\": \"Check the statement using the context.\"},\n",
    "            {\"role\":\"user\", \"content\": f\"Statement: {query_text}\\nContext:{context}\"}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
